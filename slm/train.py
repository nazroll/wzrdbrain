"""
QLoRA fine-tuning script for the WzrdBrain SLM.

Uses Unsloth for optimized 4-bit quantized training with TRL's SFTTrainer.
Trains on synthetic wizard skating combo data generated by generate_data.py.

Usage:
    python slm/train.py
    python slm/train.py --config slm/config.yaml
"""

import argparse
import json
from pathlib import Path

import yaml

SLM_DIR = Path(__file__).resolve().parent


def load_config(config_path: str | None = None) -> dict:
    """Load training configuration from YAML."""
    path = Path(config_path) if config_path else SLM_DIR / "config.yaml"
    with open(path) as f:
        return yaml.safe_load(f)


def load_dataset(filepath: Path) -> list[dict]:
    """Load a JSONL dataset file."""
    examples = []
    with open(filepath) as f:
        for line in f:
            examples.append(json.loads(line))
    return examples


def main() -> None:
    parser = argparse.ArgumentParser(description="Train WzrdBrain SLM")
    parser.add_argument("--config", type=str, default=None, help="Path to config YAML")
    parser.add_argument("--output-dir", type=str, default=None, help="Override output directory")
    args = parser.parse_args()

    config = load_config(args.config)
    model_config = config["model"]
    lora_config = config["lora"]
    train_config = config["training"]

    output_dir = args.output_dir or str(SLM_DIR / "outputs" / "wzrdbrain-slm")

    print(f"Model: {model_config['name']}")
    print(f"Output: {output_dir}")

    # ---------------------------------------------------------------------------
    # 1. Load model with Unsloth (4-bit quantized)
    # ---------------------------------------------------------------------------
    from unsloth import FastLanguageModel

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_config["name"],
        max_seq_length=model_config["max_seq_length"],
        dtype=None,  # auto-detect
        load_in_4bit=model_config["load_in_4bit"],
    )

    # ---------------------------------------------------------------------------
    # 2. Apply LoRA adapters
    # ---------------------------------------------------------------------------
    model = FastLanguageModel.get_peft_model(
        model,
        r=lora_config["r"],
        lora_alpha=lora_config["alpha"],
        lora_dropout=lora_config["dropout"],
        target_modules=lora_config["target_modules"],
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=train_config["seed"],
    )

    # ---------------------------------------------------------------------------
    # 3. Load and format training data
    # ---------------------------------------------------------------------------
    train_data = load_dataset(SLM_DIR / config["data"]["train_file"])
    val_data = load_dataset(SLM_DIR / config["data"]["val_file"])

    print(f"Train examples: {len(train_data)}")
    print(f"Val examples: {len(val_data)}")

    # Convert to HF chat format using the tokenizer's chat template
    def format_example(example: dict) -> str:
        """Apply the model's chat template to format messages."""
        return tokenizer.apply_chat_template(
            example["messages"],
            tokenize=False,
            add_generation_prompt=False,
        )

    from datasets import Dataset

    train_dataset = Dataset.from_list(train_data)
    val_dataset = Dataset.from_list(val_data)

    # Add formatted text column
    train_dataset = train_dataset.map(
        lambda x: {"text": format_example(x)}, remove_columns=["messages"]
    )
    val_dataset = val_dataset.map(
        lambda x: {"text": format_example(x)}, remove_columns=["messages"]
    )

    # ---------------------------------------------------------------------------
    # 4. Configure trainer
    # ---------------------------------------------------------------------------
    from trl import SFTTrainer
    from transformers import TrainingArguments

    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=train_config["num_epochs"],
        per_device_train_batch_size=train_config["per_device_batch_size"],
        gradient_accumulation_steps=train_config["gradient_accumulation_steps"],
        learning_rate=train_config["learning_rate"],
        lr_scheduler_type=train_config["lr_scheduler"],
        warmup_ratio=train_config["warmup_ratio"],
        weight_decay=train_config["weight_decay"],
        max_grad_norm=train_config["max_grad_norm"],
        logging_steps=train_config["logging_steps"],
        save_steps=train_config["save_steps"],
        eval_strategy="steps",
        eval_steps=train_config["eval_steps"],
        seed=train_config["seed"],
        bf16=True,
        optim="adamw_8bit",
        report_to="none",
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
    )

    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        args=training_args,
        dataset_text_field="text",
        max_seq_length=model_config["max_seq_length"],
        packing=False,
    )

    # ---------------------------------------------------------------------------
    # 5. Train
    # ---------------------------------------------------------------------------
    print("\nStarting training...")
    trainer_stats = trainer.train()

    print(f"\nTraining complete!")
    print(f"  Total steps: {trainer_stats.global_step}")
    print(f"  Training loss: {trainer_stats.training_loss:.4f}")

    # Save the LoRA adapter
    lora_output = str(Path(output_dir) / "lora")
    model.save_pretrained(lora_output)
    tokenizer.save_pretrained(lora_output)
    print(f"  LoRA adapter saved to: {lora_output}")

    # Also save in merged 16-bit format for easier deployment
    merged_output = str(Path(output_dir) / "merged")
    model.save_pretrained_merged(merged_output, tokenizer, save_method="merged_16bit")
    print(f"  Merged model saved to: {merged_output}")


if __name__ == "__main__":
    main()
