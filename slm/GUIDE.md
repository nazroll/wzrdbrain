# WzrdBrain SLM — Training & Deployment Guide

A step-by-step guide to fine-tune and deploy a small language model that generates valid wizard skating trick combos. Each phase is self-contained — you can stop after any phase and pick up later.

**What you'll end up with**: A Qwen 2.5 1.5B model fine-tuned on ~9k synthetic examples, hosted on Hugging Face with a Gradio chat demo.

**Time estimate**: ~2-3 hours hands-on, plus ~1-2 hours of GPU training time.

---

## Prerequisites

- A [Hugging Face account](https://huggingface.co/join) with a write access token
- Python 3.10+
- Git + this repo cloned locally
- A Hugging Face payment method or free Colab GPU (the training step needs a GPU — your laptop CPU won't work)

### Local setup

```bash
cd wzrdbrain
python -m venv .venv
source .venv/bin/activate
pip install -e ".[dev]"
pip install -r slm/requirements.txt
```

### Hugging Face auth

```bash
pip install huggingface_hub
huggingface-cli login
```

Paste your HF token when prompted. This token is used in Phases 3-5 for pulling the base model and pushing the fine-tuned model.

---

## Phase 1: Knowledge Curation

**Goal**: Define the domain knowledge the model needs to learn beyond raw combo generation.

**Files**: `slm/knowledge/trick_descriptions.json`, `slm/knowledge/learning_progressions.json`

These are already written. Review them to make sure descriptions match reality:

```bash
# Quick sanity check — should list all 26 moves
python -c "import json; d=json.load(open('slm/knowledge/trick_descriptions.json')); print(f'{len(d)} moves'); print(list(d.keys()))"
```

**To edit**: Open `trick_descriptions.json` directly. Each move has `description`, `difficulty` (beginner/intermediate/advanced/expert), `category`, `notes`, and `rules`. The `learning_progressions.json` groups moves into 6 difficulty tiers.

**If you add new moves to `tricks.json`**: Update both knowledge files to include them, then re-run Phase 2.

---

## Phase 2: Generate Training Data

**Goal**: Create ~9,000 validated training examples from the wzrdbrain library itself. Every combo in the dataset is generated by the real `generate_combo()` function, so the training data is guaranteed valid.

```bash
python slm/generate_data.py
```

**Expected output**:
```
Generating synthetic training data for WzrdBrain SLM...
  A. Generating 5000 simple combo examples...
  B. Generating 3000 constrained combo examples...
  C. Generating 500 trick explanation examples...
  D. Generating 300 rule explanation examples...
  E. Generating 200 progression examples...

Total examples: 9000
Valid examples: 9000
Train: 7920, Val: 880, Test: 200
```

**Output files** (in `slm/data/`, gitignored):
| File | Records | Purpose |
|------|---------|---------|
| `train.jsonl` | ~7,920 | Training set |
| `val.jsonl` | ~880 | Validation (eval during training) |
| `test.jsonl` | 200 | Held-out test set for Phase 4 |

Each line is a JSON object in HuggingFace chat format:

```json
{
  "messages": [
    {"role": "system", "content": "You are WzrdBrain, an expert wizard skating..."},
    {"role": "user", "content": "Generate a 3-trick combo."},
    {"role": "assistant", "content": "Here's a 3-trick combo:\n\n1. front open gazelle\n2. back closed tree\n3. back closed lion s"}
  ]
}
```

**Inspect samples manually**:
```bash
# Print 5 random examples
python -c "
import json, random
data = [json.loads(l) for l in open('slm/data/train.jsonl')]
for ex in random.sample(data, 5):
    print(f\"User: {ex['messages'][1]['content']}\")
    print(f\"Asst: {ex['messages'][2]['content'][:120]}...\")
    print()
"
```

**Tuning the data mix**: Edit `generation:` counts in `slm/config.yaml` and re-run. More combo examples improve generation accuracy; more explanation examples improve conversational quality.

---

## Phase 3: Fine-Tune the Model

**Goal**: QLoRA fine-tune Qwen 2.5 1.5B Instruct on the generated data.

This is the only phase that requires a GPU. You have three options:

### Option A: Hugging Face Training (Recommended)

The simplest path — use HF's infrastructure directly.

**1. Upload the dataset to Hugging Face**:

```bash
python -c "
from datasets import load_dataset
from huggingface_hub import HfApi

# Load local JSONL files
ds = load_dataset('json', data_files={
    'train': 'slm/data/train.jsonl',
    'validation': 'slm/data/val.jsonl',
    'test': 'slm/data/test.jsonl',
})
ds.push_to_hub('nazroll/wzrdbrain-slm-data', private=True)
print('Dataset uploaded!')
"
```

**2. Create a training job on HF Spaces with a GPU runtime**:

Go to [huggingface.co/new-space](https://huggingface.co/new-space):
- **SDK**: Gradio or Docker
- **Hardware**: T4 small (free tier works) or A10G (faster, ~$1/hr)
- Upload `slm/train.py` and `slm/config.yaml`

Or, more practically, use a **Colab notebook** that connects to HF (see Option B).

**3. Run training directly if you have a local GPU**:

```bash
python slm/train.py
```

### Option B: Google Colab (Free GPU)

Open a Colab notebook and run these cells:

```python
# Cell 1: Setup
!pip install unsloth trl peft datasets pyyaml accelerate bitsandbytes -q

# Cell 2: Clone repo and install wzrdbrain
!git clone https://github.com/YOUR_USERNAME/wzrdbrain.git
%cd wzrdbrain
!pip install -e ".[dev]" -q

# Cell 3: Generate data (or upload pre-generated data)
!python slm/generate_data.py

# Cell 4: Train
!python slm/train.py
```

Training takes ~4-6 hours on a T4, ~1-2 hours on an A100.

### Option C: HF AutoTrain (No-Code Alternative)

If you want to skip writing training code entirely:

1. Upload the dataset to HF Hub (step 1 from Option A)
2. Go to [huggingface.co/autotrain](https://huggingface.co/autotrain)
3. Select **LLM Fine-tuning**
4. Set base model to `Qwen/Qwen2.5-1.5B-Instruct`
5. Point it at your `nazroll/wzrdbrain-slm-data` dataset
6. Use these settings:
   - LoRA r: 16, alpha: 32
   - Epochs: 3
   - Learning rate: 2e-4
   - Batch size: 4, gradient accumulation: 4

AutoTrain handles QLoRA, merging, and pushing to Hub automatically.

### What to expect during training

The training script logs every 10 steps. Watch for:

```
Step  10 | Loss: 2.3451
Step  20 | Loss: 1.8234
Step  50 | Loss: 1.2456
Step 100 | Loss: 0.9123
...
```

**Loss should steadily decrease**. If it plateaus above 1.5 or spikes, something is wrong — check that data files exist and the model downloaded correctly.

**Output** (in `slm/outputs/wzrdbrain-slm/`):
- `lora/` — LoRA adapter weights (small, ~50MB)
- `merged/` — Full merged model (large, ~3GB)

---

## Phase 4: Evaluate

**Goal**: Measure rule compliance on generated combos. This tells you whether the model actually learned wizard skating rules or is just pattern-matching.

```bash
python slm/evaluate.py --model-path slm/outputs/wzrdbrain-slm/merged
```

If you used AutoTrain or pushed to Hub already:
```bash
python slm/evaluate.py --model-path nazroll/wzrdbrain-slm
```

**Expected output**:
```
============================================================
WzrdBrain SLM Evaluation Results
============================================================
  Parseable output                         97.0%  (target: 95%)  [PASS]
  Valid moves (no hallucinations)          99.2%  (target: 99%)  [PASS]
  ONLY_FIRST rule compliance              99.5%  (target: 99%)  [PASS]
  Direction linking                        98.3%  (target: 98%)  [PASS]
============================================================
  Overall: ALL PASS
============================================================
```

Save detailed results for review:
```bash
python slm/evaluate.py --model-path slm/outputs/wzrdbrain-slm/merged --output slm/eval_results.json
```

### What the metrics mean

| Metric | What it checks | Why it matters |
|--------|---------------|----------------|
| **Parseable output** | Model produces a numbered list (`1. trick name`) | If the model rambles instead of listing tricks, it's useless |
| **Valid moves** | Every move name exists in the 26-move vocabulary | Hallucinated move names (e.g., "dragon spin") break downstream consumers |
| **ONLY_FIRST** | Predator/predator one/parallel never appear after trick 1 | Core combo rule from `tricks.json` |
| **Direction linking** | `trick[i].exit == trick[i+1].enter` | The fundamental mechanic of wizard skating combos |

### If metrics are below targets

- **Below 95% parseable**: Add more simple combo examples (Category A), increase to 7,000+
- **Below 99% valid moves**: The model is hallucinating. Train for more epochs (try 5) or increase data
- **Below 99% ONLY_FIRST**: Add explicit negative examples showing why predator can't be mid-combo
- **Below 98% direction linking**: This is the hardest rule. Increase constrained examples (Category B) and add more detailed combo examples that show the direction chain explicitly
- **All metrics bad**: Check that training loss converged below 1.0. If not, the model may need a different learning rate — try 1e-4

---

## Phase 5: Deploy to Hugging Face

### 5a. Push the model to Hub

```bash
python slm/push_to_hub.py --model-path slm/outputs/wzrdbrain-slm/lora
```

Add GGUF exports for local inference (llama.cpp, Ollama, etc.):
```bash
python slm/push_to_hub.py --model-path slm/outputs/wzrdbrain-slm/lora --gguf
```

This uploads:
- Merged 16-bit model weights
- Tokenizer
- Model card (auto-generated README)
- GGUF variants: `Q4_K_M` (~900MB, good quality) and `Q8_0` (~1.6GB, near-lossless)

**Result**: Model live at `https://huggingface.co/nazroll/wzrdbrain-slm`

### 5b. Deploy the Gradio demo to HF Spaces

**1. Create the Space**:

```bash
# Create a new HF Space
huggingface-cli repo create wzrdbrain-slm-demo --type space --space-sdk gradio

# Clone it
git clone https://huggingface.co/spaces/nazroll/wzrdbrain-slm-demo /tmp/wzrdbrain-demo
cd /tmp/wzrdbrain-demo

# Copy the Space files
cp /path/to/wzrdbrain/slm/space/app.py .
cp /path/to/wzrdbrain/slm/space/requirements.txt .

# Push
git add -A && git commit -m "Initial Space deploy" && git push
```

**2. Configure hardware** (on huggingface.co/spaces/nazroll/wzrdbrain-slm-demo → Settings):
- **Hardware**: T4 small (for GPU inference) or CPU basic (slower but free)
- The model is 1.5B params — it runs on CPU, just slower (~5s per response vs <1s on GPU)

**3. Verify**: Visit `https://huggingface.co/spaces/nazroll/wzrdbrain-slm-demo` and try:
- "Generate a 3-trick combo"
- "What is a gazelle?"
- "I'm a beginner, what should I learn first?"

---

## Quick Reference

| Phase | Command | Time | Needs GPU? |
|-------|---------|------|-----------|
| 1. Knowledge | Edit `slm/knowledge/*.json` | 15 min | No |
| 2. Data gen | `python slm/generate_data.py` | < 1 min | No |
| 3. Train | `python slm/train.py` | 1-6 hrs | **Yes** |
| 4. Evaluate | `python slm/evaluate.py --model-path ...` | ~10 min | Yes (inference) |
| 5. Deploy | `python slm/push_to_hub.py --model-path ...` | ~15 min | Yes (merge) |

### Config cheat sheet (`slm/config.yaml`)

| Setting | Default | What it controls |
|---------|---------|-----------------|
| `model.name` | `Qwen/Qwen2.5-1.5B-Instruct` | Base model. Try `Qwen/Qwen2.5-0.5B-Instruct` for smaller |
| `lora.r` | 16 | LoRA rank. Higher = more capacity but slower training |
| `training.num_epochs` | 3 | More epochs = better fit but risk overfitting |
| `training.learning_rate` | 2e-4 | Standard for QLoRA. Lower (1e-4) if loss is unstable |
| `generation.*_count` | varies | Number of examples per category |

### Troubleshooting

| Problem | Fix |
|---------|-----|
| `OutOfMemoryError` during training | Reduce `per_device_batch_size` to 2 (and increase `gradient_accumulation_steps` to 8) |
| `ImportError: unsloth` | Run `pip install unsloth` — requires a CUDA GPU |
| Training loss doesn't decrease | Check that data files exist in `slm/data/`. Try regenerating with `python slm/generate_data.py` |
| Model outputs gibberish | Likely undertrained. Increase epochs to 5 or increase dataset size |
| GGUF export fails | Requires `llama-cpp-python`. Install with `pip install llama-cpp-python` |
| Space shows "Building" forever | Check Space logs in the HF UI for missing dependencies |
